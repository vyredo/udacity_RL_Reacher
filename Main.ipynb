{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "Congratulations for completing the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program!  In this notebook, you will learn how to control an agent in a more challenging environment, where the goal is to train a creature with four arms to walk forward.  **Note that this exercise is optional!**\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Crawler.app\"`\n",
    "- **Windows** (x86): `\"path/to/Crawler_Windows_x86/Crawler.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Crawler_Windows_x86_64/Crawler.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Crawler_Linux/Crawler.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Crawler_Linux/Crawler.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Crawler_Linux_NoVis/Crawler.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Crawler_Linux_NoVis/Crawler.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Crawler.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Crawler.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['PATH'] = f\"{os.environ['PATH']}:/home/student/.local/bin\"\n",
    "# os.environ['PATH'] = f\"{os.environ['PATH']}:/opt/conda/lib/python3.10/site-packages\"\n",
    "\n",
    "os.environ['PATH'] = f\"{os.environ['PATH']}:/home/vidy/.local/bin\"\n",
    "os.environ['PATH'] = f\"{os.environ['PATH']}:/home/vidy/mambaforge/envs/py310/lib/python3.10/site-packages\"\n",
    "\n",
    "\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip freeze | grep numpy\n",
    "# !pip -q install . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP ENVIRONMENT \n",
    "(The environment embedded in this repository is only for Linux 20 agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found path: /home/vidy/RL_Reacher/Reacher.x86_64\n",
      "Mono path[0] = '/home/vidy/RL_Reacher/Reacher_Data/Managed'\n",
      "Mono config path = '/home/vidy/RL_Reacher/Reacher_Data/MonoBleedingEdge/etc'\n",
      "Preloaded 'ScreenSelector.so'\n",
      "Preloaded 'libgrpc_csharp_ext.x64.so'\n",
      "Unable to preload the following plugins:\n",
      "\tScreenSelector.so\n",
      "\tlibgrpc_csharp_ext.x86.so\n",
      "Logging to /home/vidy/.config/unity3d/Unity Technologies/Unity Environment/Player.log\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "## Setting up Environment  \n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# Path to the Unity environment binary \n",
    "# (THE File Env PROVIDED IS ONLY FOR LINUX), feel free to replace with other env\n",
    "env_path = \"Reacher.x86_64\"\n",
    "\n",
    "env = UnityEnvironment(file_name=env_path, no_graphics=True)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True, )[brain_name]\n",
    "\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save Report\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def save_report(scores, end_times, log_file=\"training_logs.txt\", folder=\"Report\"):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # 1. Plot and save scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(scores, label=\"Scores\")\n",
    "    plt.axhline(y=np.mean(scores[-100:]), color=\"r\", linestyle=\"--\", label=\"Last 100 Average\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Training Progress\")\n",
    "    plt.legend()\n",
    "    plot_path = os.path.join(folder, \"training_progress.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Score plot saved: {plot_path}\")\n",
    "\n",
    "    log_path = os.path.join(folder, log_file)\n",
    "    with open(log_path, \"w\") as f:\n",
    "        for i, score in enumerate(scores):\n",
    "            rounded_values = [round(v, 2) for v in [score, end_times[i]]]\n",
    "            score, end_episode = rounded_values\n",
    "            f.write(f\"Episode: {i+1} average last 100 Score: {score}, done in {end_episode} seconds\\n\")\n",
    "    print(f\"Logs saved: {log_path}\")\n",
    "    \n",
    "def save_model(agent, folder=\"Report\"):\n",
    "    # 1. Save the model\n",
    "    actor_path = os.path.join(folder, \"actor.pth\")\n",
    "    critic_path = os.path.join(folder, \"critic.pth\")\n",
    "    torch.save(agent.actor_local.state_dict(), actor_path)\n",
    "    torch.save(agent.critic_local.state_dict(), critic_path)\n",
    "    print(f\"Models saved: {actor_path}, {critic_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Global variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from Agent import DDPGAgent\n",
    "\n",
    "BUFFER_SIZE = int(1e8)  # replay buffer size\n",
    "BATCH_SIZE = 256        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor\n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "DEVICE = 'cuda'\n",
    "USE_PER = False\n",
    "use_mixed_precision= False # if your GPU support mixed precision in NVIDIA\n",
    "\n",
    "## if using PER, uncomment below otherwise commentout when using REplayBuffer\n",
    "## PER takes longer time to complete 1 epidsode and the learning is not consistent\n",
    "# USE_PER = True\n",
    "# BUFFER_SIZE = int(1e4)\n",
    "# BATCH_SIZE = 128\n",
    "\n",
    "agent = DDPGAgent(\n",
    "    DEVICE=DEVICE,\n",
    "    BUFFER_SIZE=BUFFER_SIZE,\n",
    "    BATCH_SIZE=BATCH_SIZE,\n",
    "    GAMMA=GAMMA,\n",
    "    TAU=TAU,\n",
    "    LR_ACTOR=LR_ACTOR,\n",
    "    LR_CRITIC=LR_CRITIC,\n",
    "    WEIGHT_DECAY=WEIGHT_DECAY,\n",
    "    state_size=state_size, action_size=action_size, random_seed=0,\n",
    "    use_mixed_precision=use_mixed_precision,\n",
    "    USE_PER=USE_PER)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for episode 1 step 0: 0.00, calculated in 0.04s\n",
      "Scores for episode 1 step 100: 0.07, calculated in 1.49s\n",
      "Scores for episode 1 step 200: 0.25, calculated in 1.72s\n",
      "Scores for episode 1 step 300: 0.38, calculated in 1.81s\n",
      "Scores for episode 1 step 400: 0.49, calculated in 1.81s\n",
      "Scores for episode 1 step 500: 0.58, calculated in 1.89s\n",
      "Scores for episode 1 step 600: 0.67, calculated in 1.95s\n",
      "Scores for episode 1 step 700: 0.78, calculated in 1.94s\n",
      "Scores for episode 1 step 800: 0.88, calculated in 1.97s\n",
      "Scores for episode 1 step 900: 0.97, calculated in 2.15s\n",
      "\n",
      "Episode 1\tAverage Score: 1.13, finished in 18.730684647001908s\n",
      "Score plot saved: Report/training_progress.png\n",
      "Logs saved: Report/training_logs.txt\n",
      "\n",
      "Environment solved in 1 episodes!, finished in 18.730684647001908 seconds \n",
      "Models saved: Report/actor.pth, Report/critic.pth\n",
      "Scores for episode 2 step 0: 0.00, calculated in 0.00s\n",
      "Scores for episode 2 step 100: 0.10, calculated in 2.00s\n",
      "Scores for episode 2 step 200: 0.22, calculated in 2.04s\n",
      "Scores for episode 2 step 300: 0.28, calculated in 2.03s\n",
      "Scores for episode 2 step 400: 0.35, calculated in 1.98s\n",
      "Scores for episode 2 step 500: 0.36, calculated in 1.96s\n",
      "Scores for episode 2 step 600: 0.40, calculated in 1.99s\n",
      "Scores for episode 2 step 700: 0.42, calculated in 2.02s\n",
      "Scores for episode 2 step 800: 0.50, calculated in 1.92s\n",
      "Scores for episode 2 step 900: 0.55, calculated in 1.97s\n",
      "\n",
      "Episode 2\tAverage Score: 0.69, finished in 19.90087756599678s\n",
      "Score plot saved: Report/training_progress.png\n",
      "Logs saved: Report/training_logs.txt\n",
      "\n",
      "Environment solved in 2 episodes!, finished in 19.90087756599678 seconds \n",
      "Models saved: Report/actor.pth, Report/critic.pth\n",
      "Scores for episode 3 step 0: 0.00, calculated in 0.00s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "max_steps_per_episode = 1000       \n",
    "scores = []  \n",
    "end_times = []\n",
    "scores_window = deque(maxlen=100)  \n",
    "i_episode = 0\n",
    "max_episode = 2000 # allow for x episode running instead of running forever\n",
    "\n",
    "while True:\n",
    "    if(i_episode > max_episode): break\n",
    "    \n",
    "    i_episode += 1\n",
    "    env_info = env.reset(train_mode=True)[brain_name]  \n",
    "    states = env_info.vector_observations            \n",
    "    agent.reset()                                     \n",
    "    episode_scores = np.zeros(20)\n",
    "    start_episode = time.perf_counter()\n",
    "    # Initialize episode score\n",
    "    \n",
    "    start_step = time.perf_counter()\n",
    "    for t in range(max_steps_per_episode):\n",
    "        actions = agent.act(states)                     \n",
    "        env_info = env.step(actions)[brain_name]       \n",
    "        next_states = env_info.vector_observations  \n",
    "        rewards = env_info.rewards                   \n",
    "        dones = env_info.local_done                 \n",
    "\n",
    "        # Save experience and learn\n",
    "        if(t % 100 == 0):        \n",
    "            end_step = time.perf_counter() - start_step\n",
    "            start_step = time.perf_counter()\n",
    "            print(f\"Scores for episode {i_episode} step {t}: {np.mean(episode_scores):.2f}, calculated in {end_step:.2f}s\")\n",
    "        for i in range(20):\n",
    "            agent.step(states[i], actions[i], rewards[i], next_states[i], dones[i], t)\n",
    "\n",
    "        # Transition to next state\n",
    "        states = next_states                           \n",
    "        episode_scores += rewards                             \n",
    "\n",
    "        if np.any(dones): \n",
    "            break\n",
    "\n",
    "    avg_score = np.mean(episode_scores)\n",
    "    scores.append(avg_score)\n",
    "    scores_window.append(avg_score)\n",
    "    end_episode = time.perf_counter() - start_episode\n",
    "    end_times.append(end_episode)\n",
    "    \n",
    "    #save model and report\n",
    "    print(f\"\\nEpisode {i_episode}\\tAverage Score: {avg_score:.2f}, finished in {end_episode}s\")\n",
    "    save_report(scores, end_times)\n",
    "    print(f\"\\nEnvironment solved in {i_episode} episodes!, finished in {end_episode} seconds \")\n",
    "    save_model(agent)\n",
    "    \n",
    "    \n",
    "    # Gradually reduce noise\n",
    "    if hasattr(agent.noise, 'sigma'):\n",
    "        agent.noise.sigma = max(0.1, agent.noise.sigma * 0.995)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
